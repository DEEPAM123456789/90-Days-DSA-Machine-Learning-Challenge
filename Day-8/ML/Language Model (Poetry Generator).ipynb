{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9208833d-0770-43f7-9f4f-d5faf183720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93c07ce-a1a9-45d8-9ec4-8147f36f9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6dd397-368f-44af-b07a-ea16c03f493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} # start of a phrase\n",
    "first_order = {} # second word only\n",
    "second_order = {} # stores 2nd-order transitions: (word1, word2) -> next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63b15eb-9904-46ea-b848-36d4eec29e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b65c75d-9bea-471c-8bc3-9f5482c7ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Robert Frost ===\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth; \n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim\n",
      "Because it was grassy and wanted wear,\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves no step had trodden black.\n",
      "Oh, I kept the first for another day! \n",
      "Yet knowing how way \n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\Deepam\\Projects\\Text Classifier\\robert_frost.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    frost_text = f.read()\n",
    "\n",
    "print(\"\\n=== Robert Frost ===\")\n",
    "print(frost_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9203e03-a47a-4283-a09d-18d94cb0209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)\n",
    "\n",
    "# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56b56e-12be-4149-b9c2-44281743f719",
   "metadata": {},
   "source": [
    "ðŸ”¸ Purpose:\n",
    "This helper function helps us build a dictionary of lists.\n",
    "\n",
    "Each key k points to a list of values v.\n",
    "\n",
    "ðŸ”¸ How it works:\n",
    "Letâ€™s say we want to track the animals that come after each animal in a story. Something like this:\n",
    "```python\n",
    "cat cat dog dog dog dog dog mouse\n",
    "```\n",
    "Weâ€™ll use add2dict like this:\n",
    "```python\n",
    "d = {}\n",
    "animals = ['cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'dog', 'mouse']\n",
    "for i in range(1, len(animals)):\n",
    "    prev = animals[i - 1]\n",
    "    curr = animals[i]\n",
    "    add2dict(d, prev, curr)\n",
    "    \n",
    "Now what happens:\n",
    "\n",
    "i\tprev\tcurr\tdict after add2dict\n",
    "1\tcat\tcat\t{'cat': ['cat']}\n",
    "2\tcat\tdog\t{'cat': ['cat', 'dog']}\n",
    "3\tdog\tdog\t{'cat': [...], 'dog': ['dog']}\n",
    "4\tdog\tdog\t{'dog': ['dog', 'dog']}\n",
    "5\tdog\tdog\t{'dog': ['dog', 'dog', 'dog']}\n",
    "6\tdog\tdog\t{'dog': ['dog', 'dog', 'dog', 'dog']}\n",
    "7\tdog\tmouse\t{'dog': [..., 'mouse']}\n",
    "\n",
    "Final d:\n",
    "{\n",
    "  'cat': ['cat', 'dog'],\n",
    "  'dog': ['dog', 'dog', 'dog', 'dog', 'mouse']\n",
    "}\n",
    "```\n",
    "* So add2dict(d, k, v) does:\n",
    "* If k (the key) is not in dictionary d, it adds it with an empty list.\n",
    "* It appends the value v to the list at d[k].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b61cc6-dffb-4f09-a886-8f9ffd34cee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20f4470-47fc-4c00-83f1-b0fc160f200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(r\"D:\\Deepam\\Projects\\Text Classifier\\robert_frost.txt\"):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "    T = len(tokens)\n",
    "    for i in range(T):\n",
    "        t = tokens[i]\n",
    "        if i == 0:\n",
    "            # Measure the distribution of the first word\n",
    "            initial[t] = initial.get(t, 0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[i-1]\n",
    "            if i == T - 1:\n",
    "                # measure probability of ending the line\n",
    "                add2dict(second_order, (t_1, t), \"END\")\n",
    "            if i == 1:\n",
    "                # measure distribution of second word\n",
    "                # give only first word\n",
    "                add2dict(first_order, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[i-2]\n",
    "                add2dict(second_order, (t_2, t_1), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55cc92-688f-47eb-90c3-51369ca4d63a",
   "metadata": {},
   "source": [
    "## ðŸ§  Goal of the Exercise\n",
    "\n",
    "To build a language model that can generate new lines that sound like **Robert Frost** by learning the probabilities of word sequences from his existing lines. You're building:\n",
    "\n",
    "| Component               | Description                                       |\n",
    "|------------------------|---------------------------------------------------|\n",
    "| Initial Distribution   | Which words typically begin a line?               |\n",
    "| First-Order Transition | What's likely the second word, given the first?   |\n",
    "| Second-Order Transition| What is the next word, given the previous two?    |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Variables Involved\n",
    "\n",
    "| Variable       | Purpose                                   | Example                                        |\n",
    "|----------------|-------------------------------------------|------------------------------------------------|\n",
    "| `initial`      | Counts how often a word starts a line     | `{'two': 1, 'and': 1}`                         |\n",
    "| `first_order`  | Maps 1st word â†’ possible 2nd words        | `{'two': ['roads'], 'and': ['sorry']}`        |\n",
    "| `second_order` | Maps (word1, word2) â†’ next word(s)        | `{('two', 'roads'): ['diverged']}`            |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“œ Code Breakdown\n",
    "\n",
    "```python\n",
    "for line in open(\"robert_frost.txt\"):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "```\n",
    "\n",
    "### ðŸ”¹ Step-by-Step:\n",
    "| Step         | What It Does                                               |\n",
    "|--------------|------------------------------------------------------------|\n",
    "| Read Line    | Reads each line from the file                              |\n",
    "| Clean Line   | Removes punctuation, converts to lowercase                 |\n",
    "| Tokenize     | Splits line into a list of words (tokens)                  |\n",
    "\n",
    "ðŸ“Œ **Example Line**:  \n",
    "`\"Two roads diverged in a yellow wood,\"` â†’  \n",
    "`tokens = ['two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood']`  \n",
    "`T = 7`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Loop Over Each Token\n",
    "\n",
    "```python\n",
    "T = len(tokens)\n",
    "for i in range(T):\n",
    "    t = tokens[i]\n",
    "```\n",
    "\n",
    "Looping through each word in the line using its index `i`.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Case 1: First Word (`i == 0`)\n",
    "\n",
    "```python\n",
    "if i == 0:\n",
    "    initial[t] = initial.get(t, 0.) + 1\n",
    "```\n",
    "\n",
    "Adds count for the **first word** of the line.\n",
    "\n",
    "ðŸ“Œ If the first word is `\"two\"` â†’  \n",
    "`initial['two'] = 1`\n",
    "\n",
    "Later you can normalize:\n",
    "```python\n",
    "P(w1) = count(w1) / total_lines\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Case 2: Second Word (`i == 1`)\n",
    "\n",
    "```python\n",
    "elif i == 1:\n",
    "    t_1 = tokens[i - 1]\n",
    "    add2dict(first_order, t_1, t)\n",
    "```\n",
    "\n",
    "For second word, link it to the first word.\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "```python\n",
    "tokens = ['two', 'roads', 'diverged']\n",
    "i = 1 â†’ t = 'roads', t_1 = 'two'\n",
    "â†’ first_order = {'two': ['roads']}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Case 3: Third Word & Beyond (`i >= 2`)\n",
    "\n",
    "```python\n",
    "else:\n",
    "    t_2 = tokens[i - 2]\n",
    "    t_1 = tokens[i - 1]\n",
    "    add2dict(second_order, (t_2, t_1), t)\n",
    "```\n",
    "\n",
    "For 3rd+ word, consider previous **two** words.\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "```python\n",
    "tokens = ['two', 'roads', 'diverged', 'in']\n",
    "i = 2 â†’ t = 'diverged', t_1 = 'roads', t_2 = 'two'\n",
    "â†’ second_order[('two', 'roads')] = ['diverged']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Case 4: Last Word in Line (`i == T - 1`)\n",
    "\n",
    "```python\n",
    "if i == T - 1:\n",
    "    add2dict(second_order, (t_1, t), \"END\")\n",
    "```\n",
    "\n",
    "Marks the **end of line** by appending `\"END\"` after the last two words.\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "```python\n",
    "tokens = ['two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood']\n",
    "i = 6 â†’ t = 'wood', t_1 = 'yellow'\n",
    "â†’ second_order[('yellow', 'wood')] = ['END']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ `add2dict()` Function\n",
    "\n",
    "```python\n",
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)\n",
    "```\n",
    "\n",
    "ðŸ”¹ Ensures that if key `k` is not in dictionary `d`, it initializes it as a list.  \n",
    "ðŸ”¹ Appends value `v` to the list associated with key `k`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Final Example of Dictionaries After One Line\n",
    "\n",
    "If you process the line:\n",
    "\n",
    "```\n",
    "\"Two roads diverged in a yellow wood\"\n",
    "```\n",
    "\n",
    "Your dictionaries will look like:\n",
    "\n",
    "```python\n",
    "initial = {\n",
    "    'two': 1\n",
    "}\n",
    "\n",
    "first_order = {\n",
    "    'two': ['roads']\n",
    "}\n",
    "\n",
    "second_order = {\n",
    "    ('two', 'roads'): ['diverged'],\n",
    "    ('roads', 'diverged'): ['in'],\n",
    "    ('diverged', 'in'): ['a'],\n",
    "    ('in', 'a'): ['yellow'],\n",
    "    ('a', 'yellow'): ['wood'],\n",
    "    ('yellow', 'wood'): ['END']\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a643022c-f89e-4064-8374-1bd0adf462ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b06e78b7-16a5-4e26-8354-5939bcd2104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...0\n",
    "# into {cat: 0.5, dog:0.4, mouse: 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d731d02e-e9a2-4b1c-b534-bc1677af1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2pdict(ts):\n",
    "    # turn each list of possibilities into a dictionary of probabilities\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "    for t, c in d.items():\n",
    "        d[t] = c/n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d121d-2153-4e1d-9389-77e2cc50db4f",
   "metadata": {},
   "source": [
    "* ts = ['happy', 'sad', 'happy', 'tired', 'happy', 'sad']\n",
    "* Step 1: Count frequency\n",
    "```python\n",
    "d = {\n",
    "    'happy': 3,\n",
    "    'sad': 2,\n",
    "    'tired': 1\n",
    "}\n",
    "Step 2: Normalize\n",
    "python\n",
    "Copy code\n",
    "total = 6\n",
    "\n",
    "d = {\n",
    "    'happy': 3 / 6 = 0.5,\n",
    "    'sad':   2 / 6 â‰ˆ 0.333,\n",
    "    'tired': 1 / 6 â‰ˆ 0.167\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d89b3cb3-071a-4a62-bb20-3564a5b57958",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_1, ts in first_order.items():\n",
    "    # replace list with dictionary of probabilities\n",
    "    first_order[t_1] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "660f40c8-661b-4388-abf0-dd84a8527e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, ts in second_order.items():\n",
    "    second_order[k] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53a598-92e7-4c85-82f6-2a53d6a9234d",
   "metadata": {},
   "source": [
    "* Before:\n",
    "```python\n",
    "first_order = {\n",
    "    'i': ['am', 'am', 'was'],\n",
    "    'he': ['is', 'was'],\n",
    "}\n",
    "\n",
    "second_order = {\n",
    "    ('i', 'am'): ['happy', 'tired', 'happy'],\n",
    "    ('he', 'is'): ['smart', 'intelligent', 'smart', 'lazy']\n",
    "}\n",
    "```\n",
    "* This means:\n",
    "* After the word 'i', the next word could be 'am', 'am', or 'was'\n",
    "* After 'i am', the next word could be 'happy', 'tired', or 'happy'\n",
    "\n",
    "* These are just counts (raw lists). But we want to turn them into probability distributions â€” so the model can decide what to pick next based on probabilities, not just frequency.\n",
    "\n",
    "First Order (before):\n",
    "```python\n",
    "'i': ['am', 'am', 'was']\n",
    "# list2pdict â†’ count: {'am': 2, 'was': 1}, total = 3\n",
    "# â†’ {'am': 2/3 â‰ˆ 0.666, 'was': 1/3 â‰ˆ 0.333}\n",
    "\n",
    "'he': ['is', 'was']\n",
    "# list2pdict â†’ {'is': 0.5, 'was': 0.5}\n",
    "ðŸ”¹ After transformation:\n",
    "\n",
    "first_order = {\n",
    "    'i': {'am': 0.666, 'was': 0.333},\n",
    "    'he': {'is': 0.5, 'was': 0.5}\n",
    "}\n",
    "ðŸ”¹ Second Order (before):\n",
    "\n",
    "('i', 'am'): ['happy', 'tired', 'happy']\n",
    "# â†’ {'happy': 0.666, 'tired': 0.333}\n",
    "\n",
    "('he', 'is'): ['smart', 'intelligent', 'smart', 'lazy']\n",
    "# â†’ {'smart': 0.5, 'intelligent': 0.25, 'lazy': 0.25}\n",
    "ðŸ”¹ After transformation:\n",
    "\n",
    "second_order = {\n",
    "    ('i', 'am'): {'happy': 0.666, 'tired': 0.333},\n",
    "    ('he', 'is'): {'smart': 0.5, 'intelligent': 0.25, 'lazy': 0.25}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358f59f-c34c-431c-a48d-54b52720a6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "295b5704-586c-4361-99b4-22836b626019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "    # print 'd:', d\n",
    "    p0 = np.random.random()\n",
    "    # print \"p0:\", p0\n",
    "    cumulative = 0\n",
    "    for t, p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False) # should never get here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efe0a9-d098-4698-aba8-d108ae00cb65",
   "metadata": {},
   "source": [
    "ðŸ” Example:\n",
    "```python\n",
    "d = {'am': 0.6, 'was': 0.4}\n",
    "Letâ€™s say:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "p0 = 0.42\n",
    "Now loop:\n",
    "\n",
    "t\t|p\t   |cumulative\t|Check p0 < cumulative?\t|Action\n",
    "'am'|\t0.6|\t0.6\t    |0.42 < 0.6 âœ…\t        |return am\"\n",
    "\n",
    "\n",
    "So it will return 'am'.\n",
    "\n",
    "Another case:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "p0 = 0.88\n",
    "Loop:\n",
    "\n",
    "t\t |p\t   | cumulative| Check p0 < cumulative?|\tAction\n",
    "'am' |\t0.6| 0.6\t   |0.88 < 0.6 âŒ\t       |continue\n",
    "'was'|\t0.4| 1.0\t   |0.88 < 1.0 âœ…\t       |return 'was'\n",
    "\n",
    "ðŸš¨ What about this line?\n",
    "```python\n",
    "assert(False)  # should never get here\n",
    "```\n",
    "* This is a fail-safe. It means:\n",
    "* If the loop runs completely and no word was returned, thereâ€™s a bug.\n",
    "* It should never happen if all probabilities in d sum up to 1.0 (or close enough due to float precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2195cd-9933-441b-bfdc-5117fe698baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "232ec9bb-3064-4a86-af70-787852e98622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for i in range(4): # generate 4 lines\n",
    "        sentence = []\n",
    "\n",
    "        # initial word\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "\n",
    "        # sample second word\n",
    "        w1 = sample_word(first_order[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        # second-order transitions until END\n",
    "        while True:\n",
    "            w2 = sample_word(second_order[(w0, w1)])\n",
    "            if w2 == \"END\":\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0 = w1\n",
    "            w1 = w2\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65f57dca-4e2f-41ee-9911-3085e7a0033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i know\n",
      "up to pass a winter eve\n",
      "to make them out\n",
      "and then someone\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc55b7-1e41-42fe-948b-d65e671fd551",
   "metadata": {},
   "source": [
    "* Assume this is your trained model:\n",
    "```python\n",
    "initial = {'the': 0.6, 'i': 0.4}\n",
    "first_order = {\n",
    "  'the': {'dog': 1.0},\n",
    "  'i': {'am': 1.0}\n",
    "}\n",
    "second_order = {\n",
    "  ('the', 'dog'): {'barked': 0.5, 'ran': 0.5},\n",
    "  ('dog', 'barked'): {'END': 1.0},\n",
    "  ('dog', 'ran'): {'away': 1.0},\n",
    "  ('ran', 'away'): {'END': 1.0},\n",
    "  ('i', 'am'): {'sad': 1.0},\n",
    "  ('am', 'sad'): {'END': 1.0}\n",
    "}\n",
    "```\n",
    "* Calling **generate()** might output:\n",
    "```css\n",
    "the dog ran away\n",
    "i am sad\n",
    "the dog barked\n",
    "i am sad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b44160-a59e-4088-bd73-859bb46eb64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f99df2-94a0-4d3c-a4da-9a3cc5e2a9c4",
   "metadata": {},
   "source": [
    "## Step-by-Step Implementation Plan\n",
    "* If you want to rebuild this from scratch, follow this blueprint:\n",
    "```python\n",
    "ðŸ”¹ Step 1: Load & Tokenize Text\n",
    "# Example text\n",
    "text = \"the dog ran the dog barked\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = text.lower().split()\n",
    "tokens.append(\"END\")  # mark sentence end\n",
    "ðŸ”¹ Step 2: Build Raw Frequency Dictionaries\n",
    "python\n",
    "Copy code\n",
    "initial = {}          # initial word counts\n",
    "first_order = {}      # word -> [next words]\n",
    "second_order = {}     # (word1, word2) -> [next words]\n",
    "\n",
    "# Build counts\n",
    "for i in range(len(tokens)-2):\n",
    "    t0, t1, t2 = tokens[i], tokens[i+1], tokens[i+2]\n",
    "\n",
    "    if i == 0:\n",
    "        initial[t0] = initial.get(t0, 0) + 1\n",
    "\n",
    "    if t0 not in first_order:\n",
    "        first_order[t0] = []\n",
    "    first_order[t0].append(t1)\n",
    "\n",
    "    key = (t0, t1)\n",
    "    if key not in second_order:\n",
    "        second_order[key] = []\n",
    "    second_order[key].append(t2)\n",
    "    \n",
    "ðŸ”¹ Step 3: Normalize Distributions\n",
    "\n",
    "# Normalize initial\n",
    "total = sum(initial.values())\n",
    "initial = {k: v / total for k, v in initial.items()}\n",
    "\n",
    "# Helper to convert list -> probability dict\n",
    "def list2pdict(lst):\n",
    "    d = {}\n",
    "    for word in lst:\n",
    "        d[word] = d.get(word, 0) + 1\n",
    "    total = len(lst)\n",
    "    return {k: v / total for k, v in d.items()}\n",
    "\n",
    "# Normalize first_order and second_order\n",
    "first_order = {k: list2pdict(v) for k, v in first_order.items()}\n",
    "second_order = {k: list2pdict(v) for k, v in second_order.items()}\n",
    "\n",
    "ðŸ”¹ Step 4: Define Sampling Function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for word, prob in d.items():\n",
    "        cumulative += prob\n",
    "        if p0 < cumulative:\n",
    "            return word\n",
    "    assert False, \"Should not reach here if probabilities sum to 1\"\n",
    "    \n",
    "ðŸ”¹ Step 5: Generate Sentences\n",
    "def generate():\n",
    "    for _ in range(4):  # generate 4 lines\n",
    "        sentence = []\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "\n",
    "        w1 = sample_word(first_order[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        while True:\n",
    "            w2 = sample_word(second_order[(w0, w1)])\n",
    "            if w2 == \"END\":\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0, w1 = w1, w2\n",
    "\n",
    "        print(' '.join(sentence))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b1afb-02bb-4003-bf38-2e318b31f652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2dd5843-e6a7-4b00-89d2-c6de5e700983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "#\n",
    "# Determine the vocabulary size (V)\n",
    "# We know that pi has shape V, A1 has shape V x V, and A2 has shape V x V x V\n",
    "#\n",
    "# In comparison, how many values are stored in our dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3006063-b442-4db3-97fb-19b4abe29daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2:\n",
    "# We can skip the step where we accumulate all the possible next words in a list\n",
    "# E.g. [cat, cat, dog, dog, dog, ...]\n",
    "\n",
    "# Instead, like we do with initial state distribution, create the dictionary\n",
    "# of counts directly as you loop through the data.\n",
    "#\n",
    "# You'll no longer need list2pdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbade3d7-b44e-45b9-a5dc-bd8204ea9b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
