{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b896a-27ba-4fd3-ba39-edb83ed310b2",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b2335-d839-4a4c-aedc-98a0b0018119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4beb87-38b6-479f-8bb6-c7410599d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Edgar Allan Poe ===\n",
      "LO! Death hath rear'd himself a throne\n",
      "In a strange city, all alone,\n",
      "Far down within the dim west\n",
      "Where the good, and the bad, and the worst, and the best,\n",
      "Have gone to their eternal rest.\n",
      "â€‰\n",
      "There shrines, and palaces, and towers\n",
      "Are not like any thing of ours\n",
      "Oh no! O no! ours never loom\n",
      "To heaven with that ungodly gloom!\n",
      "Time-eaten towers that tremble not!\n",
      "Resemble nothing that is ours.\n",
      "Around, by lifting winds forgot,\n",
      "Resignedly beneath the sky\n",
      "The melancholy waters lie.\n",
      "â€‰\n",
      "No holy rays from h\n",
      "\n",
      "=== Robert Frost ===\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth; \n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim\n",
      "Because it was grassy and wanted wear,\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves no step had trodden black.\n",
      "Oh, I kept the first for another day! \n",
      "Yet knowing how way \n"
     ]
    }
   ],
   "source": [
    "# Open and read Edgar Allan Poe\n",
    "with open(r\"D:\\Deepam\\Projects\\Text Classifier\\edgar_allan_poe.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    poe_text = f.read()\n",
    "\n",
    "# Open and read Robert Frost\n",
    "with open(r\"D:\\Deepam\\Projects\\Text Classifier\\robert_frost.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    frost_text = f.read()\n",
    "\n",
    "# Print first 500 characters to see the content\n",
    "print(\"=== Edgar Allan Poe ===\")\n",
    "print(poe_text[:500])\n",
    "print(\"\\n=== Robert Frost ===\")\n",
    "print(frost_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fea052c-0f08-4b1e-b5d5-c997383c0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58fac21-4c64-498f-abd9-1e5082e73071",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "    r'D:\\Deepam\\Projects\\Text Classifier\\edgar_allan_poe.txt',\n",
    "     r'D:\\Deepam\\Projects\\Text Classifier\\robert_frost.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50f2f86-34a0-4bdb-ac48-d73edbbb0804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Deepam\\Projects\\Text Classifier\\edgar_allan_poe.txt corresponds to label 0\n",
      "D:\\Deepam\\Projects\\Text Classifier\\robert_frost.txt corresponds to label 1\n"
     ]
    }
   ],
   "source": [
    "# collect data into lists\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for label, f in enumerate(input_files):\n",
    "    print(f\"{f} corresponds to label {label}\")\n",
    "\n",
    "    for line in open(f):\n",
    "        line = line.rstrip().lower()\n",
    "        if line:\n",
    "            # remove punctuation\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            input_texts.append(line)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e91617-47a0-436a-a576-e4aee8ca7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04cde9d1-f168-4c16-9fc6-0a8f90b9918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1618, 540)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ytrain), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02285e9a-3cbd-494c-b9cd-ae34ef2117d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and laugh but smile no more',\n",
       " 'a cliff and on the cliff a bottle painted',\n",
       " 'stillgoing every which way in the joints though',\n",
       " 'some shattered dishes underneath a pine',\n",
       " 'but get some color and music out of life']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad905465-c06d-4573-adca-a58a54e872e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f400172b-c21c-437f-b82e-e868e3699687",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "word2idx = {\"<unk>\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7e11c2-15f7-41ac-820b-0fa85a0eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate word2idx\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token not in word2idx:\n",
    "            word2idx[token] = idx\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c31be06e-fa7f-4982-8988-e2f1f40eb3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " 'and': 1,\n",
       " 'laugh': 2,\n",
       " 'but': 3,\n",
       " 'smile': 4,\n",
       " 'no': 5,\n",
       " 'more': 6,\n",
       " 'a': 7,\n",
       " 'cliff': 8,\n",
       " 'on': 9,\n",
       " 'the': 10,\n",
       " 'bottle': 11,\n",
       " 'painted': 12,\n",
       " 'stillgoing': 13,\n",
       " 'every': 14,\n",
       " 'which': 15,\n",
       " 'way': 16,\n",
       " 'in': 17,\n",
       " 'joints': 18,\n",
       " 'though': 19,\n",
       " 'some': 20,\n",
       " 'shattered': 21,\n",
       " 'dishes': 22,\n",
       " 'underneath': 23,\n",
       " 'pine': 24,\n",
       " 'get': 25,\n",
       " 'color': 26,\n",
       " 'music': 27,\n",
       " 'out': 28,\n",
       " 'of': 29,\n",
       " 'life': 30,\n",
       " 'shes': 31,\n",
       " 'after': 32,\n",
       " 'cider': 33,\n",
       " 'old': 34,\n",
       " 'girl': 35,\n",
       " 'thirsty': 36,\n",
       " 'if': 37,\n",
       " 'anyone': 38,\n",
       " 'had': 39,\n",
       " 'seen': 40,\n",
       " 'me': 41,\n",
       " 'coming': 42,\n",
       " 'home': 43,\n",
       " 'dreams': 44,\n",
       " 'thee': 45,\n",
       " 'therein': 46,\n",
       " 'knows': 47,\n",
       " 'are': 48,\n",
       " 'there': 49,\n",
       " 'they': 50,\n",
       " 'said': 51,\n",
       " 'it': 52,\n",
       " 'isnt': 53,\n",
       " 'going': 54,\n",
       " 'to': 55,\n",
       " 'rain': 56,\n",
       " 'how': 57,\n",
       " 'horrible': 58,\n",
       " 'monody': 59,\n",
       " 'floats': 60,\n",
       " 'rather': 61,\n",
       " 'than': 62,\n",
       " 'tip': 63,\n",
       " 'table': 64,\n",
       " 'for': 65,\n",
       " 'you': 66,\n",
       " 'let': 67,\n",
       " 'arose': 68,\n",
       " 'with': 69,\n",
       " 'duplicate': 70,\n",
       " 'horn': 71,\n",
       " 'our': 72,\n",
       " 'talk': 73,\n",
       " 'been': 74,\n",
       " 'serious': 75,\n",
       " 'sober': 76,\n",
       " 'dont': 77,\n",
       " 'ask': 78,\n",
       " 'joe': 79,\n",
       " 'her': 80,\n",
       " 'early': 81,\n",
       " 'leafs': 82,\n",
       " 'flower': 83,\n",
       " 'that': 84,\n",
       " 'much': 85,\n",
       " 'sell': 86,\n",
       " 'theyre': 87,\n",
       " 'worth': 88,\n",
       " 'as': 89,\n",
       " 'keep': 90,\n",
       " 'while': 91,\n",
       " 'i': 92,\n",
       " 'am': 93,\n",
       " 'dead': 94,\n",
       " 'yet': 95,\n",
       " 'alive': 96,\n",
       " 'were': 97,\n",
       " 'their': 98,\n",
       " 'strength': 99,\n",
       " 'subdued': 100,\n",
       " 'wont': 101,\n",
       " 'see': 102,\n",
       " 'any': 103,\n",
       " 'mormon': 104,\n",
       " 'swimming': 105,\n",
       " 'johns': 106,\n",
       " 'bad': 107,\n",
       " 'farmer': 108,\n",
       " 'im': 109,\n",
       " 'not': 110,\n",
       " 'blaming': 111,\n",
       " 'him': 112,\n",
       " 'wasnt': 113,\n",
       " 'always': 114,\n",
       " 'hudsons': 115,\n",
       " 'bay': 116,\n",
       " 'its': 117,\n",
       " 'greatly': 118,\n",
       " 'afraid': 119,\n",
       " 'fire': 120,\n",
       " 'lifts': 121,\n",
       " 'existence': 122,\n",
       " 'plane': 123,\n",
       " 'snow': 124,\n",
       " 'time': 125,\n",
       " 'entombed': 126,\n",
       " 'all': 127,\n",
       " 'fresh': 128,\n",
       " 'sound': 129,\n",
       " 'from': 130,\n",
       " 'recent': 131,\n",
       " 'axe': 132,\n",
       " 'he': 133,\n",
       " 'fell': 134,\n",
       " 'made': 135,\n",
       " 'lantern': 136,\n",
       " 'rattle': 137,\n",
       " 'say': 138,\n",
       " 'consideration': 139,\n",
       " 'better': 140,\n",
       " 'banking': 141,\n",
       " 'trade': 142,\n",
       " 'or': 143,\n",
       " 'leases': 144,\n",
       " 'perhaps': 145,\n",
       " 'she': 146,\n",
       " 'will': 147,\n",
       " 'come': 148,\n",
       " 'still': 149,\n",
       " 'unafraid': 150,\n",
       " 'o': 151,\n",
       " 'god': 152,\n",
       " 'my': 153,\n",
       " 'funereal': 154,\n",
       " 'mind': 155,\n",
       " 'cant': 156,\n",
       " 'explain': 157,\n",
       " 'other': 158,\n",
       " 'two': 159,\n",
       " 'words': 160,\n",
       " 'foreign': 161,\n",
       " 'soft': 162,\n",
       " 'dissyllables': 163,\n",
       " 'now': 164,\n",
       " 'mockery': 165,\n",
       " 'boast': 166,\n",
       " 'grass': 167,\n",
       " 'may': 168,\n",
       " 'thrive': 169,\n",
       " 'wait': 170,\n",
       " 'until': 171,\n",
       " 'give': 172,\n",
       " 'hand': 173,\n",
       " 'up': 174,\n",
       " 'ledges': 175,\n",
       " 'show': 176,\n",
       " 'lines': 177,\n",
       " 'ruled': 178,\n",
       " 'southeastnorthwest': 179,\n",
       " 'such': 180,\n",
       " 'few': 181,\n",
       " 'people': 182,\n",
       " 'hadnt': 183,\n",
       " 'gnawed': 184,\n",
       " 'hitching': 185,\n",
       " 'posts': 186,\n",
       " 'bill': 187,\n",
       " 'whose': 188,\n",
       " 'heartstrings': 189,\n",
       " 'lute': 190,\n",
       " 'waves': 191,\n",
       " 'have': 192,\n",
       " 'redder': 193,\n",
       " 'glow': 194,\n",
       " 'at': 195,\n",
       " 'his': 196,\n",
       " 'shoulders': 197,\n",
       " 'dragging': 198,\n",
       " 'yellow': 199,\n",
       " 'strands': 200,\n",
       " 'passed': 201,\n",
       " 'by': 202,\n",
       " 'watchman': 203,\n",
       " 'beat': 204,\n",
       " 'jupiter': 205,\n",
       " 'very': 206,\n",
       " 'air': 207,\n",
       " 'what': 208,\n",
       " 'we': 209,\n",
       " 'ride': 210,\n",
       " 'one': 211,\n",
       " 'earth': 212,\n",
       " 'ever': 213,\n",
       " 'find': 214,\n",
       " 'gone': 215,\n",
       " 'so': 216,\n",
       " 'far': 217,\n",
       " 'youve': 218,\n",
       " 'heard': 219,\n",
       " 'estelles': 220,\n",
       " 'run': 221,\n",
       " 'off': 222,\n",
       " 'gushing': 223,\n",
       " 'strange': 224,\n",
       " 'tears': 225,\n",
       " 'riddle': 226,\n",
       " 'your': 227,\n",
       " 'genealogy': 228,\n",
       " 'done': 229,\n",
       " 'is': 230,\n",
       " 'facts': 231,\n",
       " 'want': 232,\n",
       " 'go': 233,\n",
       " 'henceforth': 234,\n",
       " 'hold': 235,\n",
       " 'thy': 236,\n",
       " 'flowerenameled': 237,\n",
       " 'shore': 238,\n",
       " 'spare': 239,\n",
       " 'them': 240,\n",
       " 'trouble': 241,\n",
       " 'double': 242,\n",
       " 'troubles': 243,\n",
       " 'braced': 244,\n",
       " 'feet': 245,\n",
       " 'against': 246,\n",
       " 'arctic': 247,\n",
       " 'pole': 248,\n",
       " 'many': 249,\n",
       " 'thoughts': 250,\n",
       " 'hopes': 251,\n",
       " 'strangely': 252,\n",
       " 'anything': 253,\n",
       " 'wish': 254,\n",
       " 'like': 255,\n",
       " 'wild': 256,\n",
       " 'geese': 257,\n",
       " 'lake': 258,\n",
       " 'before': 259,\n",
       " 'storm': 260,\n",
       " 'bells': 261,\n",
       " 'bowed': 262,\n",
       " 'grace': 263,\n",
       " 'natural': 264,\n",
       " 'law': 265,\n",
       " 'sometimes': 266,\n",
       " 'gets': 267,\n",
       " 'possessed': 268,\n",
       " 'accounts': 269,\n",
       " 'supposed': 270,\n",
       " 'be': 271,\n",
       " 'mad': 272,\n",
       " 'likely': 273,\n",
       " 'regard': 274,\n",
       " 'sacred': 275,\n",
       " 'misting': 276,\n",
       " 'lets': 277,\n",
       " 'fair': 278,\n",
       " 'Ã¢â‚¬â€°': 279,\n",
       " 'gnawing': 280,\n",
       " 'till': 281,\n",
       " 'whined': 282,\n",
       " 'house': 283,\n",
       " 'pipes': 284,\n",
       " 'smoking': 285,\n",
       " 'jug': 286,\n",
       " 'was': 287,\n",
       " 'halted': 288,\n",
       " 'something': 289,\n",
       " 'brushed': 290,\n",
       " 'across': 291,\n",
       " 'passionate': 292,\n",
       " 'light': 293,\n",
       " 'spirit': 294,\n",
       " 'fit': 295,\n",
       " 'ah': 296,\n",
       " 'night': 297,\n",
       " 'nights': 298,\n",
       " 'year': 299,\n",
       " 'myrtle': 300,\n",
       " 'blade': 301,\n",
       " 'ill': 302,\n",
       " 'entwine': 303,\n",
       " 'getting': 304,\n",
       " 'again': 305,\n",
       " 'because': 306,\n",
       " 'long': 307,\n",
       " 'ago': 308,\n",
       " 'nearest': 309,\n",
       " 'resembles': 310,\n",
       " 'worship': 311,\n",
       " 'oh': 312,\n",
       " 'remember': 313,\n",
       " 'save': 314,\n",
       " 'only': 315,\n",
       " 'heaven': 316,\n",
       " 'when': 317,\n",
       " 'did': 318,\n",
       " 'chores': 319,\n",
       " 'fine': 320,\n",
       " 'fibrils': 321,\n",
       " 'strains': 322,\n",
       " 'too': 323,\n",
       " 'these': 324,\n",
       " 'years': 325,\n",
       " 'stardials': 326,\n",
       " 'hinted': 327,\n",
       " 'morn': 328,\n",
       " 'writer': 329,\n",
       " 'hall': 330,\n",
       " 'finished': 331,\n",
       " 'bedroom': 332,\n",
       " 'witchs': 333,\n",
       " 'motto': 334,\n",
       " 'anyway': 335,\n",
       " 'isle': 336,\n",
       " 'fairest': 337,\n",
       " 'flowers': 338,\n",
       " 'fervid': 339,\n",
       " 'flickering': 340,\n",
       " 'torch': 341,\n",
       " 'lit': 342,\n",
       " 'just': 343,\n",
       " 'ought': 344,\n",
       " 'enough': 345,\n",
       " 'protection': 346,\n",
       " 'sadly': 347,\n",
       " 'this': 348,\n",
       " 'star': 349,\n",
       " 'mistrust': 350,\n",
       " 'must': 351,\n",
       " 'however': 352,\n",
       " 'books': 353,\n",
       " 'conquered': 354,\n",
       " 'scruples': 355,\n",
       " 'gloom': 356,\n",
       " 'lying': 357,\n",
       " 'forward': 358,\n",
       " 'weakly': 359,\n",
       " 'handrail': 360,\n",
       " 'woman': 361,\n",
       " 'afterward': 362,\n",
       " 'stained': 363,\n",
       " 'vegetation': 364,\n",
       " 'above': 365,\n",
       " 'being': 366,\n",
       " 'mounted': 367,\n",
       " 'bareback': 368,\n",
       " 'introduce': 369,\n",
       " 'among': 370,\n",
       " 'buttons': 371,\n",
       " 'poured': 372,\n",
       " 'lap': 373,\n",
       " 'best': 374,\n",
       " 'right': 375,\n",
       " 'place': 376,\n",
       " 'reason': 377,\n",
       " 'property': 378,\n",
       " 'saw': 379,\n",
       " 'stay': 380,\n",
       " 'unless': 381,\n",
       " 'stove': 382,\n",
       " 'happen': 383,\n",
       " 'prefer': 384,\n",
       " 'live': 385,\n",
       " 'bowers': 386,\n",
       " 'whereat': 387,\n",
       " 'outward': 388,\n",
       " 'mountain': 389,\n",
       " 'halfway': 390,\n",
       " 'rivers': 391,\n",
       " 'glide': 392,\n",
       " 'sorrowfully': 393,\n",
       " 'trailed': 394,\n",
       " 'dust': 395,\n",
       " 'winter': 396,\n",
       " 'evening': 397,\n",
       " 'walk': 398,\n",
       " 'senescent': 399,\n",
       " 'holy': 400,\n",
       " 'rays': 401,\n",
       " 'down': 402,\n",
       " 'olden': 403,\n",
       " 'ceasing': 404,\n",
       " 'hymns': 405,\n",
       " 'attend': 406,\n",
       " 'spell': 407,\n",
       " 'agony': 408,\n",
       " 'sobbed': 409,\n",
       " 'letting': 410,\n",
       " 'sink': 411,\n",
       " 'know': 412,\n",
       " 'raining': 413,\n",
       " 'mother': 414,\n",
       " 'lane': 415,\n",
       " 'might': 416,\n",
       " 'married': 417,\n",
       " 'nothing': 418,\n",
       " 'came': 419,\n",
       " 'sailed': 420,\n",
       " 'once': 421,\n",
       " 'thing': 422,\n",
       " 'help': 423,\n",
       " 'liking': 424,\n",
       " 'about': 425,\n",
       " 'john': 426,\n",
       " 'laurels': 427,\n",
       " 'belong': 428,\n",
       " 'taken': 429,\n",
       " 'thus': 430,\n",
       " 'into': 431,\n",
       " 'family': 432,\n",
       " 'lay': 433,\n",
       " 'got': 434,\n",
       " 'goodlooking': 435,\n",
       " 'picked': 436,\n",
       " 'allowed': 437,\n",
       " 'take': 438,\n",
       " 'upside': 439,\n",
       " 'start': 440,\n",
       " 'lately': 441,\n",
       " 'slept': 442,\n",
       " 'apathy': 443,\n",
       " 'ghoulhaunted': 444,\n",
       " 'woodland': 445,\n",
       " 'weir': 446,\n",
       " 'then': 447,\n",
       " 'leaf': 448,\n",
       " 'subsides': 449,\n",
       " 'since': 450,\n",
       " 'flickers': 451,\n",
       " 'through': 452,\n",
       " 'why': 453,\n",
       " 'preyest': 454,\n",
       " 'thou': 455,\n",
       " 'upon': 456,\n",
       " 'poets': 457,\n",
       " 'heart': 458,\n",
       " 'most': 459,\n",
       " 'would': 460,\n",
       " 'think': 461,\n",
       " 'expense': 462,\n",
       " 'guess': 463,\n",
       " 'estelle': 464,\n",
       " 'filled': 465,\n",
       " 'purse': 466,\n",
       " 'troop': 467,\n",
       " 'echoes': 468,\n",
       " 'sweet': 469,\n",
       " 'duty': 470,\n",
       " 'little': 471,\n",
       " 'horse': 472,\n",
       " 'queer': 473,\n",
       " 'went': 474,\n",
       " 'sleep': 475,\n",
       " 'bed': 476,\n",
       " 'idled': 477,\n",
       " 'hes': 478,\n",
       " 'glass': 479,\n",
       " 'case': 480,\n",
       " 'wander': 481,\n",
       " 'beaten': 482,\n",
       " 'ways': 483,\n",
       " 'listen': 484,\n",
       " 'lean': 485,\n",
       " 'therefore': 486,\n",
       " 'art': 487,\n",
       " 'wrong': 488,\n",
       " 'yes': 489,\n",
       " 'stark': 490,\n",
       " 'mossy': 491,\n",
       " 'banks': 492,\n",
       " 'meandering': 493,\n",
       " 'paths': 494,\n",
       " 'joy': 495,\n",
       " 'wo': 496,\n",
       " 'good': 497,\n",
       " 'left': 498,\n",
       " 'an': 499,\n",
       " 'open': 500,\n",
       " 'door': 501,\n",
       " 'cool': 502,\n",
       " 'room': 503,\n",
       " 'ultimate': 504,\n",
       " 'climes': 505,\n",
       " 'same': 506,\n",
       " 'seize': 507,\n",
       " 'catscradle': 508,\n",
       " 'strings': 509,\n",
       " 'soul': 510,\n",
       " 'lest': 511,\n",
       " 'should': 512,\n",
       " 'truant': 513,\n",
       " 'violin': 514,\n",
       " 'name': 515,\n",
       " 'sure': 516,\n",
       " 'werent': 517,\n",
       " 'under': 518,\n",
       " 'ones': 519,\n",
       " 'hands': 520,\n",
       " 'gold': 521,\n",
       " 'pay': 522,\n",
       " 'birth': 523,\n",
       " 'silver': 524,\n",
       " 'beaded': 525,\n",
       " 'fur': 526,\n",
       " 'having': 527,\n",
       " 'interfered': 528,\n",
       " 'huse': 529,\n",
       " 'business': 530,\n",
       " 'pearly': 531,\n",
       " 'lustre': 532,\n",
       " 'moon': 533,\n",
       " 'israfeli': 534,\n",
       " 'who': 535,\n",
       " 'despisest': 536,\n",
       " 'mustnt': 537,\n",
       " 'belilaced': 538,\n",
       " 'cellar': 539,\n",
       " 'hole': 540,\n",
       " 'luminous': 541,\n",
       " 'windows': 542,\n",
       " 'city': 543,\n",
       " 'alone': 544,\n",
       " 'daylight': 545,\n",
       " 'beauty': 546,\n",
       " 'over': 547,\n",
       " 'creaking': 548,\n",
       " 'wanted': 549,\n",
       " 'put': 550,\n",
       " 'never': 551,\n",
       " 'takes': 552,\n",
       " 'money': 553,\n",
       " 'loved': 554,\n",
       " 'object': 555,\n",
       " 'tear': 556,\n",
       " 'lid': 557,\n",
       " 'myself': 558,\n",
       " 'end': 559,\n",
       " 'vista': 560,\n",
       " 'blend': 561,\n",
       " 'turrets': 562,\n",
       " 'shadows': 563,\n",
       " 'both': 564,\n",
       " 'morning': 565,\n",
       " 'equally': 566,\n",
       " 'gems': 567,\n",
       " 'thought': 568,\n",
       " 'turn': 569,\n",
       " 'upturnd': 570,\n",
       " 'faces': 571,\n",
       " 'roses': 572,\n",
       " 'rights': 573,\n",
       " 'temptation': 574,\n",
       " 'do': 575,\n",
       " 'realms': 576,\n",
       " 'boreal': 577,\n",
       " 'sometime': 578,\n",
       " 'hourly': 579,\n",
       " 'hope': 580,\n",
       " 'staid': 581,\n",
       " 'shelter': 582,\n",
       " 'farm': 583,\n",
       " 'endless': 584,\n",
       " 'ages': 585,\n",
       " 'shall': 586,\n",
       " 'cherish': 587,\n",
       " 'fame': 588,\n",
       " 'stirred': 589,\n",
       " 'abysses': 590,\n",
       " 'firsts': 591,\n",
       " 'attic': 592,\n",
       " 'throw': 593,\n",
       " 'bare': 594,\n",
       " 'legs': 595,\n",
       " 'theyd': 596,\n",
       " 'drag': 597,\n",
       " 'call': 598,\n",
       " 'nausicaa': 599,\n",
       " 'mean': 600,\n",
       " 'miles': 601,\n",
       " 'settlement': 602,\n",
       " 'happy': 603,\n",
       " 'repining': 604,\n",
       " 'trees': 605,\n",
       " 'trod': 606,\n",
       " 'uncomfortably': 607,\n",
       " 'crunching': 608,\n",
       " 'round': 609,\n",
       " 'glory': 610,\n",
       " 'shut': 611,\n",
       " 'tell': 612,\n",
       " 'back': 613,\n",
       " 'us': 614,\n",
       " 'looked': 615,\n",
       " 'course': 616,\n",
       " 'world': 617,\n",
       " 'gentle': 618,\n",
       " 'stamped': 619,\n",
       " 'things': 620,\n",
       " 'himself': 621,\n",
       " 'youd': 622,\n",
       " 'tempt': 623,\n",
       " 'bird': 624,\n",
       " 'singing': 625,\n",
       " 'kept': 626,\n",
       " 'remembering': 627,\n",
       " 'bard': 628,\n",
       " 'wisest': 629,\n",
       " 'each': 630,\n",
       " 'hour': 631,\n",
       " 'bid': 632,\n",
       " 'stately': 633,\n",
       " 'palace': 634,\n",
       " 'young': 635,\n",
       " 'used': 636,\n",
       " 'roar': 637,\n",
       " 'angels': 638,\n",
       " 'dreaming': 639,\n",
       " 'moonlit': 640,\n",
       " 'dew': 641,\n",
       " 'shine': 642,\n",
       " 'bright': 643,\n",
       " 'eyes': 644,\n",
       " 'wonder': 645,\n",
       " 'doesnt': 646,\n",
       " 'marry': 647,\n",
       " 'could': 648,\n",
       " 'recognize': 649,\n",
       " 'post': 650,\n",
       " 'spoiled': 651,\n",
       " 'day': 652,\n",
       " 'began': 653,\n",
       " 'uncertain': 654,\n",
       " 'disturbed': 655,\n",
       " 'doubt': 656,\n",
       " 'even': 657,\n",
       " 'meridian': 658,\n",
       " 'glare': 659,\n",
       " 'faroff': 660,\n",
       " 'happier': 661,\n",
       " 'sea': 662,\n",
       " 'bathe': 663,\n",
       " 'crystalline': 664,\n",
       " 'keeps': 665,\n",
       " 'cash': 666,\n",
       " 'where': 667,\n",
       " 'can': 668,\n",
       " 'well': 669,\n",
       " 'havent': 670,\n",
       " 'brought': 671,\n",
       " 'fountain': 672,\n",
       " 'man': 673,\n",
       " 'found': 674,\n",
       " 'id': 675,\n",
       " 'names': 676,\n",
       " 'everything': 677,\n",
       " 'mica': 678,\n",
       " 'sheets': 679,\n",
       " 'big': 680,\n",
       " 'plateglass': 681,\n",
       " 'visions': 682,\n",
       " 'consult': 683,\n",
       " 'voices': 684,\n",
       " 'governor': 685,\n",
       " 'proclaimed': 686,\n",
       " 'seem': 687,\n",
       " 'short': 688,\n",
       " 'hog': 689,\n",
       " 'reeve': 690,\n",
       " 'march': 691,\n",
       " 'meeting': 692,\n",
       " 'here': 693,\n",
       " 'warren': 694,\n",
       " 'whom': 695,\n",
       " 'thine': 696,\n",
       " 'absence': 697,\n",
       " 'wings': 698,\n",
       " 'theres': 699,\n",
       " 'keeping': 700,\n",
       " 'lo': 701,\n",
       " 'stir': 702,\n",
       " 'lighting': 703,\n",
       " 'lonely': 704,\n",
       " 'pathway': 705,\n",
       " 'fix': 706,\n",
       " 'alley': 707,\n",
       " 'titanic': 708,\n",
       " 'youth': 709,\n",
       " 'known': 710,\n",
       " 'smell': 711,\n",
       " 'wet': 712,\n",
       " 'feathers': 713,\n",
       " 'heat': 714,\n",
       " 'incredulous': 715,\n",
       " 'own': 716,\n",
       " 'luck': 717,\n",
       " 'set': 718,\n",
       " 'lot': 719,\n",
       " 'record': 720,\n",
       " 'discordant': 721,\n",
       " 'melody': 722,\n",
       " 'melancholy': 723,\n",
       " 'shrine': 724,\n",
       " 'whats': 725,\n",
       " 'hurry': 726,\n",
       " 'hell': 727,\n",
       " 'unhitch': 728,\n",
       " 'saved': 729,\n",
       " 'news': 730,\n",
       " 'carryÃ¢â‚¬â€if': 731,\n",
       " 'seeking': 732,\n",
       " 'new': 733,\n",
       " 'hampshire': 734,\n",
       " 'neednt': 735,\n",
       " 'matter': 736,\n",
       " 'grassy': 737,\n",
       " 'wear': 738,\n",
       " 'natures': 739,\n",
       " 'first': 740,\n",
       " 'green': 741,\n",
       " 'less': 742,\n",
       " 'dim': 743,\n",
       " 'floor': 744,\n",
       " 'despite': 745,\n",
       " 'lion': 746,\n",
       " 'likeness': 747,\n",
       " 'surprise': 748,\n",
       " 'thrilly': 749,\n",
       " 'tourist': 750,\n",
       " 'french': 751,\n",
       " 'indian': 752,\n",
       " 'esquimaux': 753,\n",
       " 'granny': 754,\n",
       " 'speaking': 755,\n",
       " 'dunnow': 756,\n",
       " 'living': 757,\n",
       " 'youre': 758,\n",
       " 'frogs': 759,\n",
       " 'peeping': 760,\n",
       " 'thousand': 761,\n",
       " 'shrill': 762,\n",
       " 'nor': 763,\n",
       " 'ive': 764,\n",
       " 'brown': 765,\n",
       " 'standing': 766,\n",
       " 'cold': 767,\n",
       " 'everybody': 768,\n",
       " 'took': 769,\n",
       " 'proof': 770,\n",
       " 'west': 771,\n",
       " 'voice': 772,\n",
       " 'mute': 773,\n",
       " 'saint': 774,\n",
       " 'mark': 775,\n",
       " 'says': 776,\n",
       " 'prove': 777,\n",
       " 'high': 778,\n",
       " 'girdle': 779,\n",
       " 'wheels': 780,\n",
       " 'hang': 781,\n",
       " 'bade': 782,\n",
       " 'pause': 783,\n",
       " 'gardengate': 784,\n",
       " 'cut': 785,\n",
       " 'sorry': 786,\n",
       " 'travel': 787,\n",
       " 'robinsons': 788,\n",
       " 'guide': 789,\n",
       " 'groping': 790,\n",
       " 'jam': 791,\n",
       " 'except': 792,\n",
       " 'johnjoe': 793,\n",
       " 'point': 794,\n",
       " 'path': 795,\n",
       " 'skies': 796,\n",
       " 'pull': 797,\n",
       " 'ladder': 798,\n",
       " 'road': 799,\n",
       " 'behind': 800,\n",
       " 'love': 801,\n",
       " 'near': 802,\n",
       " 'paradise': 803,\n",
       " 'pairing': 804,\n",
       " 'ends': 805,\n",
       " 'darkly': 806,\n",
       " 'present': 807,\n",
       " 'past': 808,\n",
       " 'weak': 809,\n",
       " 'written': 810,\n",
       " 'looking': 811,\n",
       " 'another': 812,\n",
       " 'try': 813,\n",
       " 'stock': 814,\n",
       " 'counts': 815,\n",
       " 'ideals': 816,\n",
       " 'threw': 817,\n",
       " 'wetelbowed': 818,\n",
       " 'wetkneed': 819,\n",
       " 'position': 820,\n",
       " 'handsÃ¢â‚¬â€': 821,\n",
       " 'son': 822,\n",
       " 'story': 823,\n",
       " 'twas': 824,\n",
       " 'setting': 825,\n",
       " 'traps': 826,\n",
       " 'wouldnt': 827,\n",
       " 'grew': 828,\n",
       " 'ashen': 829,\n",
       " 'chisel': 830,\n",
       " 'work': 831,\n",
       " 'enormous': 832,\n",
       " 'glacier': 833,\n",
       " 'times': 834,\n",
       " 'warrant': 835,\n",
       " 'hurt': 836,\n",
       " 'someone': 837,\n",
       " 'doing': 838,\n",
       " 'replied': 839,\n",
       " 'ulalume': 840,\n",
       " 'burned': 841,\n",
       " 'stake': 842,\n",
       " 'faith': 843,\n",
       " 'godliness': 844,\n",
       " 'throne': 845,\n",
       " 'thrones': 846,\n",
       " 'longforgotten': 847,\n",
       " 'exclaimed': 848,\n",
       " 'afar': 849,\n",
       " 'else': 850,\n",
       " 'amid': 851,\n",
       " 'earthly': 852,\n",
       " 'moans': 853,\n",
       " 'theme': 854,\n",
       " 'praise': 855,\n",
       " 'town': 856,\n",
       " 'settle': 857,\n",
       " 'hence': 858,\n",
       " 'state': 859,\n",
       " 'befitting': 860,\n",
       " 'journeyed': 861,\n",
       " 'soulandbody': 862,\n",
       " 'scars': 863,\n",
       " 'north': 864,\n",
       " 'south': 865,\n",
       " 'blue': 866,\n",
       " 'astartes': 867,\n",
       " 'bediamonded': 868,\n",
       " 'crescent': 869,\n",
       " 'rid': 870,\n",
       " 'those': 871,\n",
       " 'champions': 872,\n",
       " 'devoted': 873,\n",
       " 'brave': 874,\n",
       " 'toffile': 875,\n",
       " 'nails': 876,\n",
       " 'nail': 877,\n",
       " 'slap': 878,\n",
       " 'given': 879,\n",
       " 'appear': 880,\n",
       " 'headboard': 881,\n",
       " 'lifelike': 882,\n",
       " 'posture': 883,\n",
       " 'swarm': 884,\n",
       " 'hast': 885,\n",
       " 'dragged': 886,\n",
       " 'diana': 887,\n",
       " 'car': 888,\n",
       " 'sybilic': 889,\n",
       " 'splendor': 890,\n",
       " 'beaming': 891,\n",
       " 'lookoff': 892,\n",
       " 'faced': 893,\n",
       " 'radiant': 894,\n",
       " 'reared': 895,\n",
       " 'head': 896,\n",
       " 'threatener': 897,\n",
       " 'cloud': 898,\n",
       " 'obscured': 899,\n",
       " 'sky': 900,\n",
       " 'hed': 901,\n",
       " 'sculturd': 902,\n",
       " 'ivy': 903,\n",
       " 'stone': 904,\n",
       " 'thats': 905,\n",
       " 'mistake': 906,\n",
       " 'dear': 907,\n",
       " 'interest': 908,\n",
       " 'italian': 909,\n",
       " 'tones': 910,\n",
       " 'murmured': 911,\n",
       " 'spend': 912,\n",
       " 'outdoors': 913,\n",
       " 'meddle': 914,\n",
       " 'fate': 915,\n",
       " 'need': 916,\n",
       " 'serial': 917,\n",
       " 'ordeal': 918,\n",
       " 'cross': 919,\n",
       " 'lots': 920,\n",
       " 'walls': 921,\n",
       " 'conies': 922,\n",
       " 'sun': 923,\n",
       " 'romp': 924,\n",
       " 'trembling': 925,\n",
       " 'wire': 926,\n",
       " 'baptismal': 927,\n",
       " 'font': 928,\n",
       " 'passing': 929,\n",
       " 'almost': 930,\n",
       " 'moment': 931,\n",
       " 'opening': 932,\n",
       " 'spirits': 933,\n",
       " 'moving': 934,\n",
       " 'musically': 935,\n",
       " 'birds': 936,\n",
       " 'outer': 937,\n",
       " 'windowsill': 938,\n",
       " 'lived': 939,\n",
       " 'whole': 940,\n",
       " 'half': 941,\n",
       " 'reclining': 942,\n",
       " 'crystal': 943,\n",
       " 'london': 944,\n",
       " 'imported': 945,\n",
       " 'porphyrogene': 946,\n",
       " 'tomorrow': 947,\n",
       " 'rains': 948,\n",
       " 'knew': 949,\n",
       " 'hid': 950,\n",
       " 'spoke': 951,\n",
       " 'lettered': 952,\n",
       " 'despairs': 953,\n",
       " 'unhallowed': 954,\n",
       " 'immemorial': 955,\n",
       " 'rapid': 956,\n",
       " 'pleiads': 957,\n",
       " 'shouldnt': 958,\n",
       " 'content': 959,\n",
       " 'budinspecting': 960,\n",
       " 'presume': 961,\n",
       " 'palsied': 962,\n",
       " 'sere': 963,\n",
       " 'tempting': 964,\n",
       " 'flatness': 965,\n",
       " 'book': 966,\n",
       " 'track': 967,\n",
       " 'peoples': 968,\n",
       " 'daughters': 969,\n",
       " 'has': 970,\n",
       " 'stand': 971,\n",
       " 'small': 972,\n",
       " 'pocket': 973,\n",
       " 'folks': 974,\n",
       " 'plain': 975,\n",
       " 'outstretched': 976,\n",
       " 'dropped': 977,\n",
       " 'child': 978,\n",
       " 'lipbegotten': 979,\n",
       " 'journey': 980,\n",
       " 'thatd': 981,\n",
       " 'passer': 982,\n",
       " 'sends': 983,\n",
       " 'souls': 984,\n",
       " 'asked': 985,\n",
       " 'tall': 986,\n",
       " 'pick': 987,\n",
       " 'together': 988,\n",
       " 'wrought': 989,\n",
       " 'without': 990,\n",
       " 'breath': 991,\n",
       " 'idea': 992,\n",
       " 'deareye': 993,\n",
       " 'figures': 994,\n",
       " 'described': 995,\n",
       " 'hearthistories': 996,\n",
       " 'seemed': 997,\n",
       " 'enwritten': 998,\n",
       " 'drowned': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c5f2508-77fa-451f-bc6e-d2e1165e6cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2521"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b6813a3-82a2-4aba-b802-b25575bcd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into integer format\n",
    "train_text_int = []\n",
    "test_text_int = []\n",
    "\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    line_as_int = [word2idx[token] for token in tokens]\n",
    "    train_text_int.append(line_as_int)\n",
    "\n",
    "for text in test_text:\n",
    "    tokens = text.split()\n",
    "    line_as_int = [word2idx.get(token, 0) for token in tokens]\n",
    "    test_text_int.append(line_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781b600-b63c-4f93-94d8-adb4f1253777",
   "metadata": {},
   "source": [
    "### Why get(token,0)?\n",
    "* In the test set, you might have words never seen in training.\n",
    "* get() safely returns:\n",
    "    * the ID if found,\n",
    "    * 0 if the word is unknown (UNK token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5531c2df-864f-4207-a9fe-5f6a6454e680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[315, 92, 383, 55, 384, 55, 385],\n",
       " [10, 386, 387, 17, 44, 92, 102],\n",
       " [262, 388, 9, 10, 389, 390, 174],\n",
       " [29, 391, 392],\n",
       " [281, 50, 393, 394, 17, 10, 395]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_int[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b9c27c0-e704-44a2-aeb2-a8851a4123a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize A and pi matrices - for both classes\n",
    "V = len(word2idx)\n",
    "\n",
    "A0 = np.ones((V, V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V, V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9da6598-11e3-42c8-a59d-427b758b2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute count for A and pi\n",
    "def compute_counts(text_as_int, A, pi):\n",
    "    for tokens in text_as_int:\n",
    "        last_idx = None\n",
    "        for idx in tokens:\n",
    "            if last_idx is None:\n",
    "                # it's the first word in a sentence\n",
    "                pi[idx] += 1\n",
    "            else:\n",
    "                # the last word exists, so count a transition\n",
    "                A[last_idx, idx] += 1\n",
    "\n",
    "            # update last idx\n",
    "            last_idx = idx\n",
    "\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y==0], A0, pi0)\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y==1], A1, pi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c62caa-9235-456c-9fa8-f28a80a7285a",
   "metadata": {},
   "source": [
    "```python\n",
    "train_text_int = [\n",
    "    [0,1,2],\n",
    "    [1,2],\n",
    "    [0,2],\n",
    "    [2,1]\n",
    "]\n",
    "\n",
    "Ytrain = [0,1,0,1]\n",
    "\n",
    "```\n",
    "\n",
    "* zip(train_text_int, Ytrain) produces:\n",
    "```python\n",
    "[\n",
    "    ([0,1,2], 0),\n",
    "    ([1,2], 1),\n",
    "    ([0,2], 0),\n",
    "    ([2,1],1)\n",
    "]\n",
    "The list comprehension picks only those where y==0:\n",
    "\n",
    "[\n",
    "    [0,1,2],\n",
    "    [0,2]\n",
    "]\n",
    "So:\n",
    "\n",
    "compute_counts(\n",
    "    [\n",
    "      [0,1,2],\n",
    "      [0,2]\n",
    "    ],\n",
    "    A0,\n",
    "    pi0\n",
    ")\n",
    "```\n",
    "means:\n",
    "Count all the transitions and sentence starts for sentences in class 0.\n",
    "\n",
    "* Similarly:\n",
    "```python\n",
    "[t for t, y in zip(train_text_int, Ytrain) if y==1]\n",
    "Produces:\n",
    "\n",
    "[\n",
    "    [1,2],\n",
    "    [2,1]\n",
    "]\n",
    "So:\n",
    "\n",
    "compute_counts(\n",
    "    [\n",
    "      [1,2],\n",
    "      [2,1]\n",
    "    ],\n",
    "    A1,\n",
    "    pi1\n",
    ")\n",
    "```\n",
    "means:\n",
    "Count all the transitions and sentence starts for sentences in class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd52054-2342-4fc3-89f3-48839f3c4726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c85982-5982-4e40-acd8-db6b213aa277",
   "metadata": {},
   "source": [
    "# ***The Complete Intuition behind Text classifier and Markov's Model (Atleast read it completely once)***\n",
    "\n",
    "## ðŸ“˜ Big Picture: What are we modeling?\n",
    "\n",
    "### âœ… Goal: \n",
    "Given a sequence of words, estimate:  \n",
    "**P(sequence | class)**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Assumption:  \n",
    "The sequence follows a **Markov Chain** (bigram model):  \n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) = P(w_1) \\cdot \\prod_{t=2}^{n} P(w_t \\mid w_{t-1})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We model this **separately for each class (author)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© To estimate this for each class, we need:\n",
    "1. How likely each word is to **start a sentence** â†’ stored in **`pi`**\n",
    "2. How likely each word is to **follow any previous word** â†’ stored in **`A`**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Variables Explained\n",
    "\n",
    "### ðŸŸ¢ `pi` vectors  \n",
    "- `pi0`: counts of how often each word starts a sentence in **class 0**\n",
    "- `pi1`: counts of how often each word starts a sentence in **class 1**\n",
    "\n",
    "This is the empirical estimate of:\n",
    "\n",
    "\\[\n",
    "\\pi[i] \\propto \\text{Count(word i starts a sentence)}\n",
    "\\]\n",
    "\n",
    "**Intuition**:  \n",
    "Some authors start lines with specific words more often.  \n",
    "- Poe might start more lines with â€œTheâ€  \n",
    "- Frost might often use â€œIâ€\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ¢ `A` matrices  \n",
    "- `A0`: counts of how often each word follows another in **class 0**\n",
    "- `A1`: counts of how often each word follows another in **class 1**\n",
    "\n",
    "If vocabulary size is `V`, then:  \n",
    "- `A0` and `A1` are `V x V` matrices  \n",
    "- `A[i][j]` means:  \n",
    "  _\"How many times did word j come after word i?\"_\n",
    "\n",
    "**Intuition**:  \n",
    "This captures each authorâ€™s style of combining words.  \n",
    "- Poe might often say â€œdark nightâ€  \n",
    "- Frost might often say â€œsnow fellâ€\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŸ¢ Example in Plain English\n",
    "\n",
    "### ðŸ“„ Toy Vocabulary:\n",
    "| Index | Word  |\n",
    "|-------|-------|\n",
    "| 0     | the   |\n",
    "| 1     | night |\n",
    "| 2     | dark  |\n",
    "\n",
    "### ðŸ“š Sentences in class 0:\n",
    "1. â€œthe dark nightâ€  \n",
    "2. â€œthe nightâ€\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Step 1: Initialize counts with smoothing\n",
    "```python\n",
    "A0 = np.ones((3, 3))  # 3x3 matrix\n",
    "pi0 = np.ones(3)      # vector of size 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755164c-7778-4b85-b03e-d0007eac4bf0",
   "metadata": {},
   "source": [
    "## âœ… Step 2: Compute counts\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“„ Sentence 1: â€œthe dark nightâ€\n",
    "\n",
    "- **First word**: â€œtheâ€ â†’ index `0`  \n",
    "  â†’ `pi0[0] += 1`\n",
    "\n",
    "- **Transitions:**\n",
    "  - â€œthe â†’ darkâ€: `A0[0][2] += 1`\n",
    "  - â€œdark â†’ nightâ€: `A0[2][1] += 1`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“„ Sentence 2: â€œthe nightâ€\n",
    "\n",
    "- **First word**: â€œtheâ€  \n",
    "  â†’ `pi0[0] += 1`\n",
    "\n",
    "- **Transition:**\n",
    "  - â€œthe â†’ nightâ€: `A0[0][1] += 1`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š After processing:\n",
    "\n",
    "### `pi0` vector:\n",
    "\n",
    "[1+2, 1, 1] = [3, 1, 1]\n",
    "\n",
    "â†’ Word 0 (â€œtheâ€) started 2 sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### `A0` matrix:\n",
    "\n",
    "| from \\ to | the | night | dark |\n",
    "|-----------|-----|-------|------|\n",
    "| the       |  1  |  2    |  2   |\n",
    "| night     |  1  |  1    |  1   |\n",
    "| dark      |  1  |  2    |  1   |\n",
    "\n",
    "- From â€œtheâ€, mostly â€œnightâ€ or â€œdarkâ€ followed.\n",
    "- From â€œdarkâ€, â€œnightâ€ followed.\n",
    "- Other transitions were not seen, so they remain 1 (from smoothing).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Interpretation\n",
    "\n",
    "- `pi0`: tells you **â€œtheâ€ frequently starts sentences**.\n",
    "- `A0`: tells you which **word combinations (bigrams)** are common for that author.\n",
    "\n",
    "This is a **bigram language model learned per class**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– Why do we need them?\n",
    "\n",
    "At **prediction time**, for a **new sentence**:\n",
    "\n",
    "1. Use `pi` and `A` to **calculate the likelihood of the sequence under each class**.\n",
    "2. **Multiply all the probabilities** (with smoothing).\n",
    "3. Predict the **class with the highest overall probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš« What if you didnâ€™t compute A and pi?\n",
    "\n",
    "You would have:\n",
    "\n",
    "- No way to **estimate how sequences are formed**\n",
    "- No way to **compare which authorâ€™s style matches**\n",
    "- Your **classifier wouldnâ€™t work**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Short Summary\n",
    "\n",
    "| Concept      | Meaning                                         |\n",
    "|--------------|-------------------------------------------------|\n",
    "| `pi`         | Starting word counts                           |\n",
    "| `A`          | Word transition (bigram) counts                |\n",
    "| Each class   | Has its own `pi` and `A`                       |\n",
    "| Intuition    | This is the **Markov model** of each author's sentence structure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cf987-07d0-4eab-9b2b-0547fe61eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2690409a-092e-4f7e-9fa0-7f8f772a1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize A and pi so they are valid probability matrices\n",
    "# convince yourself that this is equivalent to the formulas shown before\n",
    "\n",
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()\n",
    "\n",
    "# keepdims = True, ensures that the sum is still two dimensional, which is required for \n",
    "# the division to broadcast correctly in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56b789-3d06-4253-9fb5-a555d6b9fdc8",
   "metadata": {},
   "source": [
    "* Example:\n",
    "* Suppose you had this counts matrix after smoothing:\n",
    "```python\n",
    "A0 = [[1, 3, 6],\n",
    "      [2, 2, 2],\n",
    "      [4, 1, 1]]\n",
    "Sum of row 0: 1+3+6=10.\n",
    "\n",
    "So row 0 becomes: [0.1, 0.3, 0.6].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7035cc-9523-4456-8188-604dc29ba2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1543847c-8c46-4439-a76e-865ee44572d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log A and pi since we don't need the actual probs\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cba2989-8125-4843-8b6f-b87a714ab088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33498145859085293, 0.6650185414091471)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute priors\n",
    "count0 = sum(y==0 for y in Ytrain)\n",
    "count1 = sum(y==1 for y in Ytrain)\n",
    "total = len(Ytrain)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "p0, p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf0c72-6cf4-4aa8-afd7-7209ac982d57",
   "metadata": {},
   "source": [
    "* Interpretation:\n",
    "    * 66.5% of the training data comes from class 0.\n",
    "    * 33.5% comes from class 1.\n",
    "* These are your priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0bc10-bf7a-403d-83e6-b08fac72d2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f50c64df-81cd-4bab-8d75-fd36aaf5cb87",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What is Maximum Likelihood?\n",
    "\n",
    "**Maximum Likelihood (ML)** says:\n",
    "\n",
    "> *\"Pick the class that makes the observed data most likely, ignoring priors.\"*\n",
    "\n",
    "**Mathematically:**\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c \\; P(data \\mid c)\n",
    "$$\n",
    "\n",
    "âœ… **Example**\n",
    "\n",
    "Imagine you have:\n",
    "\n",
    "- **Class 0:** very common phrases (high likelihood for many sentences)\n",
    "- **Class 1:** very rare phrases (low likelihood for most sentences)\n",
    "\n",
    "Suppose your priors are:\n",
    "\n",
    "- **p0 = 0.01** (very rare class 0)\n",
    "- **p1 = 0.99** (almost everything is class 1)\n",
    "\n",
    "If you **only look at likelihood**, you might still choose class 0 because it happens to fit the sentence better, even though almost nothing comes from class 0.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "This ignores the fact that class 0 is almost never seen.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ 2ï¸âƒ£ What is Posterior Probability (MAP)?\n",
    "\n",
    "**Maximum A Posteriori (MAP)** says:\n",
    "\n",
    "> *\"Pick the class with the highest posterior probability, which combines:*  \n",
    "> *Likelihood Ã— Prior.\"*\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c \\; \\underbrace{P(\\text{data} \\mid c)}_{\\text{likelihood}} \\times \\underbrace{P(c)}_{\\text{prior}}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- Even if class 0 has higher likelihood for this sentence, you still prefer class 1 if itâ€™s overall much more common.\n",
    "- This is **Bayesian thinking**â€”taking into account both:\n",
    "  - How likely the sentence is under the class model\n",
    "  - How likely you are to see that class in general\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bca7156e-c127-4899-afd2-c8bb88d5d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifer\n",
    "class Classifier:\n",
    "    def __init__(self, logAs, logpis, logpriors):\n",
    "        self.logAs = logAs\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.K = len(logpriors) # number of classes\n",
    "\n",
    "    def _compute_log_likelihood(self, input_, class_):\n",
    "        logA = self.logAs[class_]\n",
    "        logpi = self.logpis[class_]\n",
    "\n",
    "        last_idx = None\n",
    "        logprob = 0\n",
    "        for idx in input_:\n",
    "            if last_idx is None:\n",
    "                # it's the first token\n",
    "                logprob += logpi[idx]\n",
    "            else:\n",
    "                logprob += logA[last_idx, idx]\n",
    "\n",
    "            # update last_idx\n",
    "            last_idx = idx\n",
    "\n",
    "        return logprob\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for i, input_ in enumerate(inputs):\n",
    "            posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] \\\n",
    "                          for c in range(self.K)]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[i] = pred\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a15db7f7-0b88-4c16-ab68-87c352bd850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1], [logpi0, logpi1], [logp0, logp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "644f8b97-2e87-4af1-83ce-b5e471e1da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9962917181705809\n"
     ]
    }
   ],
   "source": [
    "Ptrain = clf.predict(train_text_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain == Ytrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d127816-810d-48ee-b25a-07449d0bbc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8277777777777777\n"
     ]
    }
   ],
   "source": [
    "Ptest = clf.predict(test_text_int)\n",
    "print(f\"Test acc: {np.mean(Ptest == Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a910b9-89ff-48ee-85c7-1e12440d4056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58bb59df-6b5c-4bbb-afc9-8d68f0688113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2749fca7-ffea-45f1-91f2-03df8f90243a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 536,    6],\n",
       "       [   0, 1076]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Ytrain, Ptrain)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a815d4c-c08f-4b38-8fac-070f34611caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96,  84],\n",
       "       [  9, 351]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test = confusion_matrix(Ytest, Ptest)\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c2e7ffa-a40e-4012-afd6-527185d0ce04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972196478220574"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Ytrain, Ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7b76959-b8d4-4996-80b7-72e03286ec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830188679245283"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Ytest, Ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed3853-b9c4-445d-9381-00a5820fe52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
